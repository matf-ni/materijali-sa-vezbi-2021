{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metode lokalne optimizacije bez ograničenja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matematička optimizacija bez ograničenja se vezuje za nalaženje minimuma ili maksimuma neke funkcije. Formalno govoreći, za neku funkciju $f: D \\rightarrow \\mathbb{R}$, $D \\subseteq \\mathbb{R}^n$, problem minimizacije predstavlja nalaženje vrednosti $\\min_{x \\in D} f(x)$, a problem maksimizacije nalaženje vrednosti $\\max_{x \\in D} f(x)$. Problemi minimizacije i maksimizacije su ekvivalentni u smislu da nalaženje minimuma funkcije $f$ predstavlja ujedno nalaženje maksimuma funkcije $-f$, zbog čega će u nastvaku biti reči samo o minimizaciji. Sva rešenja $x \\in D$ su dopustiva, a optimalno se smatra onim za koje je vrednost $f(x)$ najmanja kod problema minimizacije, odnosno najveća kod problema maksimizacije."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U nastavku će biti pomenute neke metode matematičke optimizacije prvog i drugog reda bez ograničenja. Metode prvog reda vrše optimizaciju na osnovu vrednosti funkcije i njenog prvog izvoda, a kod metoda drugog reda, u obzir se uzimaju i vrednosti drugog izvoda. Metoda prvog reda koju ćemo pomenuti će biti gradijentni spust, a od metoda drugog reda biće pomenut Njutnov algoritam za minimizaciju. Biće prikazana i biblitečka podrška za minimizaciju funkcije za razne slučajeve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradijentni spust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradijentni spust (eng. *gradient descent*) započinje pretragu od proizvoljno izabrane tačke $x_0$, a zatim se, kroz određen broj iteracija, naredna tačka bira krećući se u smeru suprotnom od smera gradijenta funkcije $f$.  Ako je u $k$-tom koraku izabrana tačka $x_k$, naredna tačka je određena sa $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$. Kod funkcije jedne promenljive, umesto gradijenta $\\nabla f(x_k)$, u obrascu figuriše prvi izvod $f'(x_k)$. Parametar algoritma $\\alpha$ je takozvani korak učenja (engl. learning rate) i može biti fiksiran ili se menjati kroz iteracije. U praksi, ako je njegova vrednost previše mala, sporija je konvergencija ka lokalnom minimumu, a ako je previše velika, algoritam često obilazi minimum. Algoritam se zaustavlja nakon dostignutog broja iteracija ili kada vrednost $|f(x_{k+1})-f(x_k)|$ postane manja od unapred zadate vrednosti $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, gradient, x0, alpha, eps, max_iterations):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kao primer, izvršimo metodom gradijentnog spusta nalaženje lokalnog optimuma funkcije $$f(x, y) = \\frac{1}{2}(x^2 + 10y^2)$$ za početnu vrednost $(3, 5)$. Gradijent funkcije je $\\nabla f(x, y) = (f'_x(x, y), f'_y(x, y)) = (x, 10y)$. Neka je $\\alpha = 0.1$, $\\epsilon = 0.01$ i maksimalan broj iteracija 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linijska pretraga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linijska pretraga (eng. *line search*) predstavlja nalaženje maksimalne vrednosti parametra $\\alpha$ za koju će biti izvršena minimizacija funkcije duž nekog pravca $p$. Preciznije, za neki početni vektor $x$ i funkciju $f: D \\rightarrow \\mathbb{R}$, traži se vrednost parametra $\\alpha$ tako da $f(x+\\alpha p)$ bude minimalno. Pritom se ne zahteva da se na ovaj način nađe lokalni minimum, već da se za odabranu vrednost parametra $\\alpha$ vrednost funkcije $f$ dovoljno smanji. Zbog toga se nakon linijske pretrage često može primeniti neki drugi algoritam minimizacije. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sam algoritam za pronalaženje ove vrednosti je iterativnog karaktera. Počevši od neke velike vrednosti parametra $\\alpha$ vrednost se postepeno smanjuje. Kod `backtracking` linijske pretrage koja predstavlja varijantu osnovnog algoritma tokom iteracija vrednost parametra $\\alpha$ se smanjuje tako što se množi parametrom $\\tau \\in (0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(f, x, p, tau):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Često se za kriterijum zaustavljanja prethodnog algoritma razmatra zadovoljivost Armijo pravila (engl. Armijo-Goldstein condition) koje je za pogodno izabran parametar $\\beta \\in (0, 1)$ oblika \n",
    "\n",
    "$$f(x + \\alpha p) \\le f(x) + \\beta \\alpha p^T \\nabla f(x).$$ \n",
    "\n",
    "\n",
    "\n",
    "Ovim pravilom se zapravo zahteva da se odabere takva vrednost koraka kojom se vrednost funkcije smanjuje u zadovoljavajućoj meri određenoj parametrom $\\beta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo_rule_line_search(f, x, p, gradval, tau, beta):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada ćemo modifikovati implementaciju gradijentnog spusta tako da se koristi linijska pretraga sa Armijo pravilom za određivanje parametra $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_armijo(f, gradient, x0, eps, max_iterations):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U paketu `scipy.optimize` postoji ugrađena funkcija `line_search` koja implementira linijsku pretragu ali tako da zadovoljava takozvane Volfove uslove (engl. <a href='https://en.wikipedia.org/wiki/Wolfe_conditions'> Wolfe condition </a>). Argumenti ove funkcije su: funkcija koja se posmatra, gradijent, početna tačka i pravac duž koga se ide. U nizu povratnih vrednosti značajan je prvi element, koji predstavlja traženu vrednost parametra $\\alpha$. [Ovde](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.line_search.html) možete pogledati detaljniji opis pomenute bibliotečke funkcije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Njutnova metoda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Njutnova metoda pripada metodama lokalne optimizacije drugog reda bez ograničenja, budući da osim informacije o vrednostima funkcije i njenom prvom izvodu, koristi i vrednosti drugog izvoda funkcije. Kod funkcija više promenljivih, koristi se hesijan tj. matrica drugih izvoda funkcije. Pomoću Njutnove metode se, takođe, nalazi lokalni optimum funkcije. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U slučaju funkcije jedne promenljive, krećući se od početne tačke $x_0$, u svakoj iteraciji se ažurira vrednost tačke. Ako je nakon $k$ iteracija razmatrana tačka bila $x_k$, nova tačka $x_{k+1}$ se dobija pomoću obrasca\n",
    "\n",
    "$$x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)}.$$\n",
    "\n",
    "U slučaju funkcija više promenljivih, novi vektor $x_{k+1}$ se dobija od vektora $x_k$ pomoću\n",
    "\n",
    "$$x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k),$$\n",
    "\n",
    "gde je $\\nabla f(x_k)$ gradijent, a $\\nabla^2 f(x_k)$ hesijan funkcije $f$ u tački $x_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sledećim kodom možemo implementirati Njutnovu metodu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(f, gradient, hessian, x0, eps, max_iterations):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neka je Njutnovom metodom potrebno minimizovati funkciju \n",
    "\n",
    "$$f(x, y) = \\frac{1}{2}(x-1)^2 + (x^2 - y)^2.$$\n",
    "\n",
    "u tački $(2, 2)$. Njen gradijent je\n",
    "\n",
    "$$\\nabla f(x, y) = \\begin{bmatrix}f'_x(x, y) & f'_y(x, y)\\end{bmatrix} = \\begin{bmatrix} x-1-4x(x^2-y) & 2(y-x^2)\\end{bmatrix},$$\n",
    "\n",
    "a hesijan\n",
    "\n",
    "$$\\nabla^2 f(x, y) = \\begin{bmatrix}f''_{xx}(x, y) & f''_{xy}(x, y) \\\\ f''_{yx}(x, y) & f''_{yy}(x, y) \\end{bmatrix} = \\begin{bmatrix} 12x^2-4y+1 & -4x \\\\ -4x & 2 \\end{bmatrix}.$$\n",
    "\n",
    "Uzmimo da je $\\epsilon = 10^{-4}$ i broj iteracija 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U okviru paketa `scipy.optimize` funkcija `fmin_ncg` vrši optimizaciju Njutnovom metodom. Njena prva tri argumenta su, redom, funkcija čiji minimum se traži, početna tačka i gradijent, a parametrom `fhess` se zadaje vrednost hesijana. Po završetku funkcija ispisuje status optimizacije i vraća pronađeni minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteka stavlja na raspolaganje i neke kvazi-Njutnove metode optimizacije koje ne zahtevaju izračunavanje hesijana, već koriste njegove numerički jeftinije aproksimacije. Među ovim metodama su `fmin_bfgs` i `fmin_l_bfgs_b` i obe se naslanjaju na `Broyden, Fletcher, Goldfarb & Shanno` predlog aproksimacije hesijana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
